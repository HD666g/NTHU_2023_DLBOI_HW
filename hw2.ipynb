{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HD666g/NTHU_2023_DLBOI_HW/blob/main/hw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li0bVCTuxc6n"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# National Tsing Hua University\n",
        "\n",
        "### Fall 2023\n",
        "\n",
        "#### 11210IPT 553000\n",
        "\n",
        "#### Deep Learning in Biomedical Optical Imaging\n",
        "\n",
        "## Homework 2\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "r91nuhMkrA1u"
      },
      "source": [
        "### ✏️ Task A: Transitioning to Cross-Entropy Loss (20 pts)\n",
        "\n",
        "In Lab, we utilized the **Binary Cross-Entropy (BCE) Loss** for a binary classification task. The BCE loss is articulated as:\n",
        "\n",
        "$$ \\text{BCE}(y, \\hat{y}) = - \\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right) $$\n",
        "\n",
        "Here, $y$ is the true label (0 or 1), and $\\hat{y}$ denotes the predicted probability of $y=1$.\n",
        "\n",
        "In this task, we aim to explore the implementation of a model using **Cross-Entropy (CE) Loss**, which is a more common approach for classification tasks, especially when dealing with multiple classes. CE loss is expressed as:\n",
        "\n",
        "$$ \\text{CE}(y, \\hat{y}) = -\\sum_{i} y^{(i)} \\log(\\hat{y}^{(i)}) $$\n",
        "\n",
        "In this expression, $y$ represents the ground truth labels, $ \\hat{y} $ is the predictions from your model, and $i$ is the index of the class.\n",
        "\n",
        "\n",
        "#### 1. Modify the Loss (3 pts)\n",
        "Transition to using Cross-Entropy (CE) Loss for the classification task by utilizing PyTorch's built-in functionalities. You can refer to the [official PyTorch documentation](https://pytorch.org/docs/stable/nn.html) for detailed information and guidance to ensure the correct implementation of the CE loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfJ31t4YrA1x"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Replace '...' with the appropriate loss function in PyTorch\n",
        "loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa94YtPorA13"
      },
      "source": [
        "#### 2. Modify the Model Architecture (2 pts)\n",
        "To adapt the original code for use with Cross-Entropy (CE) loss, make necessary modifications to the model architecture. Ensure it is compatible and optimized for the application of CE loss. Consider the number of output nodes and the activation function used in the output layer for effective multi-class classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuIF6HgvrA14",
        "outputId": "eb4621d6-4cfe-4744-f20a-7524c0a07bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=65536, out_features=256, bias=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Modifying the architecture to be compatible with CE loss\n",
        "ce_model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(256*256*1, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 20)\n",
        ").cuda()\n",
        "\n",
        "print(ce_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "4mDaB9r8rA16"
      },
      "source": [
        "#### 3. Reflection Questions (15 pts, 5 pts for each)\n",
        "Provide detailed answers to the questions below:\n",
        "\n",
        "**Q1. Loss Function Comparison:**  \n",
        "   What are the differences between Binary Cross-Entropy (BCE) loss and Cross-Entropy (CE) loss?\n",
        "\n",
        "**Q2. Model Architecture Modification:**  \n",
        "   What motivated the specific changes you made to the model architecture?\n",
        "\n",
        "**Q3. Adapting to CE Loss:**  \n",
        "   In the original code configured for BCE loss, two major adjustments are needed for adaptation to CE loss. Analyze and explain the necessity for these changes, referring to the code below.\n",
        "\n",
        "```python\n",
        "for images, labels in train_loader:\n",
        "    images = images.cuda()\n",
        "    images = images / 255.0\n",
        "    labels = labels.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(images)\n",
        "\n",
        "    # Change #1: Adaptation to the labels for CE loss\n",
        "    labels = labels.long()  # Changed from labels.float().unsqueeze(1) for BCE loss\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # Change #2: Predictions for CE loss\n",
        "    train_predicted = outputs.argmax(-1)  # Changed from torch.sigmoid(outputs) > 0.5 for BCE loss\n",
        "    train_correct += (train_predicted == labels).sum().item()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcFy1CF2rA18"
      },
      "source": [
        "#### Put Your Response Here:\n",
        "\n",
        "##### 1. First of all, as the question indicated, BCE is for binary classifications, so the out put will be only 0 or 1; but CE can be used for multiple classifications, so the output will be more than 0 and 1, even the output will be other kinds of expressions. So as mention above, the output layer of BCE and CE will be different, the number of ouptput layer of CE tends to be more than BCE.\n",
        "\n",
        "##### 2. Frist of all, I change the out_features to 20, it is because CE loss can be used for multi-class classification problem, so the output features can be bigger than 1.\n",
        "\n",
        "##### 3. For Change#1, CE use 'long' type label is because CE is used for multi-class classcification, so the label of CE may be like (0, 1, 2, 3...) and these labels correspond to 'long' type label;\n",
        "As for Change#2, CE use $$\\text -\\sum_{i}$$ is because, the sum of prediction of BCE is already 1, but CE due to multi-class, CE need to ensure its sum of prediction is equal to 1. And this make the result of CE can be easily interpret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxMfPcLYrA19"
      },
      "source": [
        "### ✏️ Task B: Creating an Evaluation Code (20 pts)\n",
        "\n",
        "Evaluate the performance of a pretrained deep learning model with a test dataset of chest X-ray images available in `test_normal.npy` and `test_pneumonia.npy` files. These files respectively contain 200 grayscale normal and pneumonia chest X-ray images, each of size 256×256. The objective is to calculate the model’s accuracy, defined as the percentage of images correctly classified. To accomplish this, you are tasked to write code that loads, processes, and evaluates the model on this specific dataset. Ensure each segment of code replacing the `...` placeholders is functional and aligns with the steps provided in the instructions.\n",
        "\n",
        "**Note: ⚠️ Ensure to upload your trained model's weights to your working environment if needed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo6IbovarA1_"
      },
      "source": [
        "### Step 0: Download test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vinI0FxrrA2C",
        "outputId": "7a720e75-6780-447d-c5b6-d5abeb0d0378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-14 03:56:57--  https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_normal.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13107328 (12M) [application/octet-stream]\n",
            "Saving to: ‘test_normal.npy’\n",
            "\n",
            "test_normal.npy     100%[===================>]  12.50M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-10-14 03:56:58 (246 MB/s) - ‘test_normal.npy’ saved [13107328/13107328]\n",
            "\n",
            "--2023-10-14 03:56:58--  https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_pneumonia.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13107328 (12M) [application/octet-stream]\n",
            "Saving to: ‘test_pneumonia.npy’\n",
            "\n",
            "test_pneumonia.npy  100%[===================>]  12.50M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-10-14 03:56:59 (268 MB/s) - ‘test_pneumonia.npy’ saved [13107328/13107328]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_normal.npy\n",
        "!wget https://raw.githubusercontent.com/TacoXDD/homeworks/master/dataset/test/test_pneumonia.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQVZ-_G7rA2D"
      },
      "source": [
        "### Step 1: Prepare your test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "450MQSnFrA2D",
        "outputId": "53faa069-d56e-45a1-9278-d7f63d968583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of test_abnormal: (200, 256, 256)\n",
            "Shape of test_normal: (200, 256, 256)\n",
            "Shape of x_test: (400, 256, 256)\n",
            "Shape of y_test: (400,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "test_abnormal = np.load('test_pneumonia.npy')\n",
        "test_normal = np.load('test_normal.npy')\n",
        "\n",
        "print(f'Shape of test_abnormal: {test_abnormal.shape}')\n",
        "print(f'Shape of test_normal: {test_normal.shape}')\n",
        "\n",
        "# For the data having presence of pneumonia assign 1, for the normal ones assign 0.\n",
        "test_abnormal_labels = np.ones((test_abnormal.shape[0],))\n",
        "test_normal_labels = np.zeros((test_normal.shape[0],))\n",
        "\n",
        "x_test = np.concatenate((test_abnormal, test_normal), axis=0)\n",
        "y_test = np.concatenate((test_abnormal_labels, test_normal_labels), axis=0)\n",
        "\n",
        "print(f'Shape of x_test: {x_test.shape}')\n",
        "print(f'Shape of y_test: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLCqtQKIrA2D"
      },
      "source": [
        "### Step 2: Load Test Images into PyTorch DataLoader (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxxFFGvZrA2D",
        "outputId": "7c5b88de-8591-4205-d225-c70df1ed935d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in train is 400.\n",
            "X_train: max value is 255, min value is 0, data type is torch.int64.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_test = torch.from_numpy(x_test).long()\n",
        "y_test = torch.from_numpy(y_test).long()\n",
        "\n",
        "# Combine the images and labels into a dataset\n",
        "test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "# Create a dataloader to load data in batches. Set batch size to 32.\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(f'Number of samples in train is {len(test_loader.dataset)}.')\n",
        "print(f'X_train: max value is {x_test.max().item()}, min value is {x_test.min().item()}, data type is {x_test.dtype}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-T05-JrA2E"
      },
      "source": [
        "### Step 3: Prepare Your Trained Model  (5 pts)\n",
        "- Define the architecture to match exactly with the trained model intended for inference. Ensure strict alignment to avoid errors during evaluation.\n",
        "- Load the weights from the trained model and set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjYJkwwzrA2E",
        "outputId": "f8b411e3-3848-41f9-c506-c4a370931ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-14 03:57:36--  https://github.com/HD666g/NTHU_2023_DLBOI_HW/raw/main/HW2/trained_weights.pth\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/HD666g/NTHU_2023_DLBOI_HW/main/HW2/trained_weights.pth [following]\n",
            "--2023-10-14 03:57:36--  https://raw.githubusercontent.com/HD666g/NTHU_2023_DLBOI_HW/main/HW2/trained_weights.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16821224 (16M) [application/octet-stream]\n",
            "Saving to: ‘trained_weights.pth.1’\n",
            "\n",
            "trained_weights.pth 100%[===================>]  16.04M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-10-14 03:57:36 (279 MB/s) - ‘trained_weights.pth.1’ saved [16821224/16821224]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Flatten(start_dim=1, end_dim=-1)\n",
              "  (1): Linear(in_features=65536, out_features=64, bias=True)\n",
              "  (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (3): ReLU()\n",
              "  (4): Dropout(p=0.5, inplace=False)\n",
              "  (5): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (7): ReLU()\n",
              "  (8): Dropout(p=0.5, inplace=False)\n",
              "  (9): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (11): ReLU()\n",
              "  (12): Dropout(p=0.5, inplace=False)\n",
              "  (13): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from torch import nn\n",
        "# Declare the model architecture\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "\n",
        "    nn.Linear(256*256*1, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(64, 1)\n",
        ").cuda()\n",
        "\n",
        "# Load the trained weights\n",
        "!wget https://github.com/HD666g/NTHU_2023_DLBOI_HW/raw/main/HW2/trained_weights.pth\n",
        "model.load_state_dict(torch.load('trained_weights.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHOK2EpvrA2E"
      },
      "source": [
        "### Step 4: Perform Inference and Calculate the Accuracy (10 pts)\n",
        "- Ensure the image values are processed in a manner consistent with the training phase.\n",
        "- Use the model that was trained with BCE loss to execute inference on the test dataset.\n",
        "- Note that inference should be performed in GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mUCye77srA2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9639530-da34-4ba7-d3d7-133499eeee62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is 77.75%.\n"
          ]
        }
      ],
      "source": [
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "\n",
        "        images = images.cuda()\n",
        "        images = images / 255\n",
        "\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        labels_float = labels.float().unsqueeze(1)  # Convert labels to float and match shape with outputs\n",
        "        predicted = torch.sigmoid(outputs) > 0.5\n",
        "\n",
        "        test_correct += (predicted.float() == labels_float).sum().item()\n",
        "        test_total += labels.size(0)\n",
        "test_accuracy = 100. * test_correct / test_total\n",
        "print(f'Test accuracy is {test_accuracy:.2f}%.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}